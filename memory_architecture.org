* 概论

在理想情况下，我们希望存储器容量可以无限大，这样，任何特定的 40 个二进制数集合或者字都可以立刻得到——例如，在一个比一个快速电子乘法器的操作时间还要短的时间里。假设实际上是 100 μsec 左右，因此在存储器取一个字的时间应该是 5 到 50 μsec 。同样的，字可能以相同频率被新字代替。实际上，看起来不可能达到这样的容量。因此，我们不得不去意识到构建一个存储器层次结构的可能性，其中的每一层都比上一层拥有更大的容量，但是访问速度更慢。（这是计算机中存储体系最早的蓝图）

《Preliminary Discussion of the Logical Design of an Electronic Computing Instrument》         --A.W.Burks, H.H.Goldstine,and J.von Neumann

* 引言

处理器的性能在 Moore 定律的作用下得以飞速提升。与此同时，尽管存储器的速度也在不断提高，但远远不及 CPU 速度提高得快，二者之间的差距有越来越大的趋势。如图 1 以 1980 年 CPU 和 DRAM 的性能作为基准线，可知存储器较长的访问时间已经限制了整个计算机系统性能的提高。存储结构的层次化和高速缓冲存储器 cache 的出现与应用是解决这一矛盾的必然产物。随着芯片集成度的提高，存储层次逐渐集成在微处理器的内部，如大量的片内寄存器，指令 Cache，数据 Cache，TLB，读写缓冲器以及片内存储管理等。据统计，今天的微处理器大约芯片资源的一半以上都用于片内的存储部件。随着片内存储器层次复杂度的提高，如何在微处理器上设计一个高效的多层次存储系统来缓冲处理器和存储器之间的速度差异变得越来越重要。Cache 是存储体系中的一个关键部分，片内 Cache 在明显地改善微处理器性能的同时，也占据了大量的芯片资源。

[[file:image/performance_year.jpg]]

- 图 1 CPU 与主存性能随时间变化产生的差距

目前，构建存储器层次结构主要有三种技术。主存储器由 DRAM （动态随机访问存储器） 实现，靠近处理器的那层（cache）则由 SRAM （静态随机访问存储器） 来实现。DRAM 每比特成本要低于 SRAM，但是速度比 SRAM 慢。价格的差异源于 DRAM 每比特占用的存储器空间较少，因此等量的硅制造的 DRAM 的容量会比 SRAM 的要大。速度的差异则由多种因素造成。第三种技术是磁盘（disk），它通常是存储层次结构中容量最大且速度最慢的一层。在很多嵌入式设备这，常用的闪存（flash memory）来带头磁盘。以上这些技术的访问时间和每比特的成本变化很大，如下表所示（表中使用的是 2008 年的典型数据）。
 
| Memory technology | Tyical access time | $ per GB in 2008 |
|-------------------+--------------------+------------------|
| SRAM              | 0.5-2.5 ns         | $2000-$5000      |
| DRAM              | 50-70 ns           | $20-$75          |
| Magnetic disk     | 50-70 ns           | $20-$75          |

由于这三种技术在价格和访问速度上明显各不相同，因此这三种技术对构建存储器层次结构是有好处的。如图 2 所示，在体系结构中，较快的存储器靠近处理器，而较慢的、便宜的存储器层次较低。其目的是以最低的价格向用户提供尽可能大的存储容量，同时存取速度与最快的存储器相当。数据同样被组织成层次化结构：靠近处理器的那一层中的数据是那些较远层次的子集，所有的数据被存在最慢的底层。

[[file:image/memory_heri.png]]

- 图 2 存储器层次的基本结构
  
存储器层次结构可以由多层构成，但是数据每次只能在相邻的两个层次之间进行复制。因此我们将注意力重点集中在两个层次上。高层的存储器靠近处理器，比低层存储器容量小且访问速度更快，这是因为它采用了成本更高的技术来实现。

如果处理器需要的数据存放在高层存储器中的某个块中，则称为一次命中（这就好像正好从书桌上的一本书中找到所需的信息一样）。如果在高层存储器中没有找到所需的数据，这次数据请求则称为一次缺失。随后访问低层存储器来寻找包含所需数据的那一块。命中率，或命中比率，是在高层存储器中找到数据的存储访问比例，通常被当成存储层次结构性能的一个衡量标准。缺失率（1-命中率）则是数据在高层存储器中没有找到的存储访问比例。

追求高性能是我们使用存储器层次结构的主要目的，因而命中和缺失的执行时间就显得尤为重要。命中时间是指访问存储器层次结构中的高层存储器所需要的时间，包括了判断当前访问是命中还是缺失所需的时间。缺失代价是将相应的从低层存储器替换到高层存储器中，以及将该信息块传送给处理器的时间之和，由于较高存储层次容量较小并且使用了快速的存储器部件，因此比起对存储层次中较低层的访问，命中时间要少得多，这也是缺失代价的主要组成部分。

存储器层次结构也将影响方面到一台计算机的许多其他，包括操作系统如何管理和 I/O，编译器如何产生代码，甚至对应用程序如何使用计算机也产生一定影响。当然，由于所有程序花费大量时间访问存储器，因而存储系统必然成为评估机器性能的一个主要指标。利用存储器层次结构来达到性能的提升意味着，在过去程序员可以把存储器看成是一个平台随机访问存储设备，而现在必须理解存储层次结构如何工作才能获得良好的性能。

由于存储系统对性能至关重要，计算机设计人员花费大量精力在这些系统上，并致力于开发复杂的机制来提高系统的性能。Cache 是指位于 CPU 和主存之间快速且小容量的存储器，一般由 SRAM 构成，Cache 技术极大的提高了 CPU 访问主存的平均速度，缩小了 CPU 和主存之间的速度差异，程序访问的局部性原理为 Cache 技术提供了理论基础。

*** 重点 

程序不仅表现出时间局部性，即重复使用最近被访问的数据项的趋势，同时也表现出了空间局部性，即访问与最近被访问过的数据项地址空间很近的数据项的趋势。存储器层次结构利用了时间局部性，将最近被访问的数据放在靠近处理器的地方；也利用了空间局部性，将一些包含连续字的块移至存储器层次结构的较高。

** 局部性原理

在过去的几十年里，计算机工程师们通过对计算机程序行为的研究发现：程序访问模式并不是随机的；一个程序运行过程中执行 90% 的指令仅占其全部代码的 10%[16]；程序的访存在时间上趋向于重复并且在空间上趋向于临近的地址空间。这种非随机访存的行为叫做访问的局部性。（1）如果程序访问了一个数据项，那么很有可能在不久的将来这个数据项会被再次访问；（2）如果程序访问了一个数据项，那么很有可能在不久的将来，与该数据项在地址空间上临近的数据项也会被访问到。第一种情况叫做时间局部性，第二种就叫空间局部性。

*** 时间局部性

时间局部性是指在程序的执行过程中所使用到的数据项会被反复用到的这种趋势。
这就是 Cache 背后的基本原理之一，它给了我们对数据进行管理时一种很好的启发。如
果程序要使用到一条指令或者是一个数据变量，那么将该指令或数据变量继续先保持着
便利的访问是一种很好的选择。这样就引出了 Cache，在将从主存中取出的数据项返回
CPU 的同时，还需要将该数据项的复本存储在 Cache 中。这样访存时，就不是直接去访
问主存了，而是先在 Cache 中搜索，看看是否存在相应的复本。如果存在，则直接在
Cache 对数据项进行操作；如果不存在，那么再到主存中查询，然后将数据项送往 Cache
和 CPU。

*** 空间局部性

空间局部性的出现是由于高级程序员和编译程序倾向于将相关的数据项聚集在同
一块存储空间上。这就会使程序在处理数据上像人类一样，在一段时间内集中处理一块
存储空间内的数据项。程序这种行为的经典实例体现在数组的处理上，数组中的数据一
般都是一个接着一个地被处理：元素 i 和 i+1 在存储空间上是相临的，并且元素 i+1 通
常都是在元素 i 被处理之后就会被处理。

这种行为就如时间局部性一样，也给了我们对数据进行管理时一种很好的导向，很
典型地，我们叫它前瞻算法：当程序需要使用到指令 i 或数据 i 时，这个数据项会被传
送给 CPU，同时指令 i+1 或数据 i+1 也会被从存储器中取出。这种数据管理最简单的形
式就是选择合理的数据访问粒度：这样所构建的 Cache 基本单元就叫做 Cache 块， Cache
块通常会比单个数据项要大。比如， CPU 处理数据的基本单元是 4-byte（ 1-word），那
么 Cache 块就可能会是 16-byte（ 4-word）或者 32-byte（ 8-word）等。因为 Cache 块是
原子级的，那么在 CPU 要求获取一个数据项时就会很自然地引起该数据项周围的数据
项也会被从主存中取出。

* Cache 的基本原理

Cache：一个隐藏或者存储信息的安全场所。

《Webster's New World Dictionary of the American Language》,Third College Edition,1988

** 映射方式 
因为 Cache 的存储容量远小于主存的容量，那么如何确定此次访存的内容是否存在 Cache 中呢？如果在，如何具体定位到确定的位置上，具体如下图所示

[[file:image/cache_mapped.png]]

- 图 3 映射方式

*直接映射* 是指主存中的块只能映射到 Cache 中的指定位置，不管什么时候，主存的块只能调入到 Cache 中的这个位置。若是 Cache 该位置已有数据了，那么就会出现冲突，原来 Cache 中的块将被无条件替换出去。

*全相联映射* 与直接映射形成另一个极端的映射方式称为全映射方式。其是指主存中的块可以映射到 Cache 中所有的块都满了，才会出现冲突。

*组相联映射* 介于直接映射和全相联映射方式之间。其是指将 Cache 分为若干个组，每个组中包含若干个块主存中的块采用直接映射的方式唯一的映射到 Cache 中指定的组，然后在组中使用全相联的方式再映射到组中的块。

** 块的大小
更大的 Cache 块能够更好的利用空间局部性原理以降低不命中率。如图 4 所示，增大 Cache 块的容量，通常能够降低 Cache 访问的不命中率。但在 Cache 总容量不变的前提下，随着 Cache 块容量的增长，那么 Cache 总的块数量就会下降。这就会导致主存中不同的块映射到 Cache 中同一位置的机会增大，那么 Cache 的冲突就会增大，这样反而导致 Cache 整体不命中率增加。

[[file:image/blocksize_miss.png]]

- 图 4 不命中率 vs 块大小

** Cache 容量 
Cache 容量大小同样影响着 CPU 访存的命中率和访问时间。理论上，Cache 容量越大，能够容纳主存的内容就越多，命中率应该会线性上升，但是事实并非如此。如图 5 所反映的是几种 Cache 容量及组相联与不命中率之间的关系。从图中可以看到，组相联从一路变为两路不命中率大概能有较之前 20%—30% 的改善。并且在组相联度不变的前提下，随着 Cache 容量的增加，不命中率的改善程度也越来越小。事实上，随着 Cache 容量的增大，访问 Cache 的所需的时间也有可能增加。

[[file:image/associativity.jpg]]

- 图 5 不命中率 vs 容量大小

** 替换算法
当从主存调入一个新的块到 Cache 中时，如果 Cache 当前的位置已经存在旧的块，那么就需要将旧的块替换出去，此时就需要用到替换算法来决定替换哪一个 Cache 块出去。直接映射方式不需要替换算法，因为每一个块的位置都是固定的，映射到哪一个位置，Cache 就需要替换哪一块出去。但其他两种映射方式就必然存在替换算法的问题了一个好的替换算法对于 Cache 的整体性能至关重要。下面我们来看看几种常见的替换算法：

*随机算法（RAND）:* 使用随机数产生器生成被替换的块号。

优点：简单易于实现；

缺点：没有反映程序局部性原理，命中率低。

*先进先出（FIFO）：* 选择最早调入 Cache 中的块作为被替换的块。

优点：易于实现，且利用了一定的访存历史信息；

缺点：不能正确反映程序局部性原理，命中率不高，要可能出现抖动现象。

*最近最少被使用法（LRU）：* 选择最近最少被使用的块作为被替换的块。

优点：比较正确地反映程序局部性，命中率较高；

缺点：实现比较复杂。

*最优替换算法（OPT）：* 选择将来最少使用的块作为被替换的块。

优点：命中率最高；

缺点： 不可实现，只是一种理想算法。

** 更新策略
当 CPU 在执行访存的写操作时，为了保持 Cache 和主存中数据的一致性，就涉及到了更新策略。一般的更新策略有两种：

*写直达（write-through）：* 当 CPU 执行访存写操作时，将数据同时写入 Cache 和主存。

优点：Cache 设计开销小且实现简单；

缺点：增加了将中间结果写回主存的不必要开销。

*写回（write-back）：* 当 CPU 执行访存写操作时，数据只写入 Cache，仅在需要替换块时，才将改写过的块写回主存。

优点：降低了将中间结果写回主存的不必要开销；

缺点：需要增设脏位，增加了 Cache 设计的复杂度。

另外，对于 CPU 访存写操作不命中时，还有两种选择：

*写不分配(No write-allocate) ：* 不需要将主存中相应的不命中块调入 Cache，而是只对主存进行更新。

*写分配(write-allocate) :* 先从主存中将不命中块调入 Cache，然后再进行 Cache 写命中操作。

一般来说，采用写直达策略搭配使用写不分配的方法；而写回更新策略的话，一般配合使用写分配的方法。


** 不命中
现在，让我们更深入地探讨存储体系中不命中的来源，以及当改变 Cache 的某些属性时，是怎样影响到不命中率的。对于所有的不命中来源，我们可以将其分为三类：

*强制不命中（Compulsory misses）:* 由于第一次访问 Cache 中还未被访问过的 Cache 块。这也被称为启动不命中。

*容量不命中（Capacity misses）:* 当程序执行的过程中，Cache 不能容纳得下所有程序/数据块时，当程序/数据块被替换出 Cache 后又需要恢复回来时，就导致了容量不命中。

*冲突不命中（Conflict misses）:* 在组相联或直接映射的 Caache 中，当多个程序/数据块竞争同一个组块的时候就会导致冲突不命中。这一类的不命中在全相联的 Cache 就可以得到解决。

如图 6 所示，不命中的这三大类来源与不命中率之间的关系。通过改变 Cache 设计中的某些属性，就会直接导致这些不命中的原因。因为冲突不命中的原因是由于多个程序/数据块竞争同一个组块引起的，那么通过增加 Cache 的组相联度就可以得到缓解了。然而，这样做也许会拉长 Cache 的访问时间，导致整体性能的下降。

[[file:image/cachesize_miss.jpg]]

- 图 6 不命中来源与不命中率的关系

缓和容量不命中最简单的方法就是增大 Cache 的容量。事实上，这些年，二级 Cache 的容量都在保持着稳定的增长。然而，当 Cache 的 容量达到一定程度时，我们必须要注意到 Cache 访问时间的问题了，正因为如此，一级 Cache 的容量才增加缓慢。

强制不命中是因为首次访问一个 Cache 块引起的，那么降低强制不命中的次数最基本的方法就是增大 Cache 块容量。因为程序的执行过程中指令和数据会被划分为一个 Cache 块保存在 Cache 中，那么增大了 Cache 块，就可以减少首次访问 Cache 块的次数了。然而，增大 Cache 块容量会导致不命中的代价上升，从而会给整体性能带来负面影响。当然，若你要运行的程序由数十亿条指令之多的话，那么强制不命中对程序整体的性能也就无足轻重了。

将不命中的来源分解为这三大类是一种很有用的定性模型。Cache 的设计过程中，很多设计策略之间都会相互影响，通常改变 Cache 的一个特性，都会导致不命中的其他原因。


